{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "8XzhZNGXdLKH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbNeJQIJMgch",
        "outputId": "2ef16c74-25d2-42ac-ab17-f84064e7e695"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert .parquet to .txt"
      ],
      "metadata": {
        "id": "ed9RY129REsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet('/content/drive/MyDrive/0000.parquet')\n",
        "\n",
        "# Define the path where you want to save the .txt file\n",
        "txt_file_path = '/content/drive/MyDrive/output.txt'\n",
        "\n",
        "# Write the DataFrame to a .txt file\n",
        "df.to_csv(txt_file_path, sep='\\t', index=False)\n",
        "\n",
        "print(f\"File '{txt_file_path}' created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8KeS2p4OV26",
        "outputId": "5816ad0c-7ca0-4874-8c96-257d8539ed3c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File '/content/drive/MyDrive/output.txt' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"/content/drive/My Drive/output.txt\"\n",
        "with open(text_file) as f:\n",
        "  lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "i = 0\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  i = i + 1\n",
        "  if(i==20):\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkgmB_AjPLwp",
        "outputId": "4b237669-6509-4494-cf90-e03772251be0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "english\tsinhala\n",
            "you will receive a package in the mail\tඔයාට තැපෑලෙන් පැකේජ් එකක් හම්බවේවි\n",
            "you will receive a confirmation code after completing the registration\tලියාපදිංචිය සම්පූර්ණ කලාට පස්සෙ ඔයාට තහවුරු කිරීමේ කේතයක්  හම්බවේවි\n",
            "you will receive a discount on your next purchase\tඊලඟ මිලදී ගැනීම කරන කොට ඔයාට වට්ටමක් හම්බවේවි\n",
            "you will receive a phone call from our customer service team\tඔයාට අපේ පාරිභෝගික සේවා කණ්ඩායමෙන් දුරකථන ඇමතුමක් හම්බවේවි\n",
            "you will receive a notification when your order is ready for pickup\tඔයාගේ ඇණවුම සූදානම් උනාට පස්සෙ ඔයාට දැනුම් දීමක් හම්බවේවි\n",
            "you will receive a response to your inquiry within 24 hours\tඔයාට පැය 24ක් ඇතුලත ඔයාගෙ විමසීමට පිළිතුරක්  හම්බවේවි\n",
            "you will receive a gift for your loyalty\tඔයාට ඔයාගේ පක්ෂපාතිත්වය වෙනුවෙන් තෑග්ගක් හම්බවේවි\n",
            "you will receive an invitation to the event\tඔයාට උත්සවයට ආරාධනාවක් හම්බවේවි\n",
            "you will receive a refund for the returned item\tඔයාට රිටන් කරන භාණ්ඩය වෙනුවෙන් ආපසු මුදල් ගෙවීමක් හම්බවේවි\n",
            "you'll receive an email with the details\tඔයාට විස්තර එක්ක  ඊමේල් එකක් හම්බවේවි\n",
            "you'll receive a package in the mail\tඔයාට තැපෑලෙන් පැකේජ් එකක් හම්බවේවි\n",
            "you'll receive a refund for the returned item\tඔයාට රිටන් කරන භාණ්ඩය වෙනුවෙන් ආපසු මුදල් ගෙවීමක් හම්බවේවි\n",
            "you'll receive a gift for your loyalty\tඔයාට ඔයාගේ පක්ෂපාතිත්වය වෙනුවෙන් තෑග්ගක් හම්බවේවි\n",
            "your mother will receive a car\tඔයාගෙ අම්මට කාර් එකක් හම්බවේවි\n",
            "your brother will receive a car\tඔයාගෙ සහෝදරයට කාර් එකක් හම්බවේවි\n",
            "it's pretty cool\tඒක මාර ලස්සනයි\n",
            "we're inviting you to do that\tඅපි ඔයාට ආරාධනා කරනවා ඒක කරන්න කියලා\n",
            "join us in improving machine translation\tයන්ත්‍ර පරිවර්තනය දියුණු කරන්න අපිත් එක්ක එකතු වෙන්න\n",
            "your translations will be used to train an nmt model\tඔයාගෙ පරිවර්තන NMT මොඩලය පුහුණු කරන්න පාවිච්චි කරාවි\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in range(len(lines)-10,len(lines)):\n",
        "  print(lines[x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfM_53OKQF5g",
        "outputId": "84ac0d00-2d16-42fa-ec7c-2d53f81aba3f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in a small village there lived a young girl named Lily who had a special gift for growing the most beautiful flowers\tඑක් කලෙක කුඩා ගමක ලිලී නම් තරුණියක් ජීවත් වූ අතර ඇයට ලස්සනම මල් වගා කිරීම සඳහා විශේෂ තෑග්ගක් තිබුණි\n",
            "In a distant land a brave knight named Sir Arthur embarked on a perilous journey to rescue a kidnapped princess from an evil sorcerer\tදුර රටක ශ්‍රීමත් ආතර් නම් නිර්භීත නයිට්වරයා නපුරු මායාකාරයෙකුගෙන් පැහැරගෙන ගිය කුමරියක් බේරා ගැනීමට භයානක ගමනක් ආරම්භ කළේය\n",
            "Emily a curious explorer set out on an adventure to uncover the hidden treasures of an ancient lost city deep in the Amazon rainforest\tකුතුහලය දනවන ගවේෂකයෙකු වන එමිලි ඇමසන් වනාන්තරයේ ගැඹුරින් ගිලිහී ගිය පැරණි නගරයක සැඟවුණු වස්තු සොයා ගැනීමට ත්‍රාසජනක ගමනක් ආරම්භ කළාය\n",
            "In the peaceful town of Willowbrook a mischievous cat named Oliver had a talent for getting into amusing and unexpected predicaments\tසාමකාමී නගරයක් වන විලෝබෲක්හි ඔලිවර් නම් දඟකාර බළලාට විනෝදජනක සහ අනපේක්ෂිත දුෂ්කරතාවන්ට පත්වීමේ දක්ෂතාවයක් තිබුණි\n",
            "Sarah a talented pianist dreamt of performing on the grand stage of Carnegie Hall and spent countless hours practicing her musical craft\tදක්ෂ පියානෝ වාදකයෙකු වන සාරා Carnegie Hall හි මහා වේදිකාවේ රඟ දැක්වීමට සිහින මැවූ අතර ඇගේ සංගීත ශිල්පය පුහුණු කිරීමට පැය ගණන් ගත කළාය\n",
            "Deep in the enchanted forest a group of woodland creatures led by a wise old owl embarked on a quest to save their home from destruction\tවශීකෘත වනාන්තරයේ ගැඹුරින් නුවණැති මහලු බකමූණෙකු විසින් මෙහෙයවන ලද වනාන්තර ජීවීන් කණ්ඩායමක් තම නිවස විනාශයෙන් බේරා ගැනීමේ ගවේෂණයක යෙදී සිටියහ\n",
            "On a sunny summer day a group of friends gathered at the beach for a fun-filled day of sandcastle building swimming and laughter\tඅව්ව සහිත ගිම්හාන දිනයක මිතුරන් පිරිසක් වැලි මාලිගා ගොඩ නැගීම පිහිනීම සහ සිනහවෙන් විනෝදයෙන් පිරුණු දවසක් සඳහා වෙරළට රැස් වූහ\n",
            "In a quaint seaside village a mysterious stranger arrived bringing with them an air of excitement and an unexpected twist to the townspeople's lives\tවිචිත්‍රවත් මුහුදු වෙරළේ ගම්මානයකට අද්භූත ආගන්තුකයෙකු පැමිණියේ ඔවුන් සමඟ උද්දීපනයක් සහ නගර වැසියන්ගේ ජීවිතයට අනපේක්ෂිත පෙරළියක් ගෙන එයි\n",
            "Thomas an aspiring writer found inspiration in the bustling streets of a vibrant city where every corner held a story waiting to be told\tඅභිලාෂකාමී ලේඛකයෙකු වන තෝමස් සෑම අස්සක් මුල්ලක් නෑරම කීමට බලා සිටින කතන්දර ඇති උද්යෝගිමත් නගරයක කාර්යබහුල වීදිවල ආශ්වාදයක් ලබා ගත්තේය\n",
            "In a land of mythical creatures a young dragon named Ember struggled to control her fiery breath while learning valuable lessons of friendship and courage\tමිථ්‍යා ජීවීන් සිටින රටක එම්බර් නම් තරුණ මකරෙක් මිත්‍රත්වයේ සහ ධෛර්‍යයේ වටිනා පාඩම් ඉගෙන ගනිමින් ඇගේ ගිනි හුස්ම පාලනය කිරීමට මහත් පරිශ්‍රමයක් දැරීය\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_pairs = []\n",
        "for line in lines:\n",
        "  english, sinhala = line.split(\"\\t\")\n",
        "  sinhala = \"[start] \" + sinhala + \" [end]\"\n",
        "  text_pairs.append((english, sinhala))\n",
        "\n",
        "for i in range(3):\n",
        "  print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHkY5_TBQRD1",
        "outputId": "5ebcdd30-54bd-4cf3-d17c-6d87a006a1dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('if i ever find out that you lied to me i will be very sad son', '[start] මම කවදහරි දැනගත්තොත් ඔයා මට බොරු කිව්වා කියලා ගොඩක් දුක හිතෙයි පුතේ [end]')\n",
            "('a long time ago', '[start] ගොඩක් ඉස්සර [end]')\n",
            "('you take the leg', '[start] ඔයා කකුලුවාව ගන්න [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(text_pairs)"
      ],
      "metadata": {
        "id": "sX4ULYxAQste"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the data into training,testing and validation"
      ],
      "metadata": {
        "id": "cOtmFcQbQ97n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
        "\n",
        "print(\"Total sentences:\",len(text_pairs))\n",
        "print(\"Training set size:\",len(train_pairs))\n",
        "print(\"Validation set size:\",len(val_pairs))\n",
        "print(\"Testing set size:\",len(test_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7jGW5uLRKYE",
        "outputId": "16bc9e42-7f73-4868-be5d-f88aaea1d8db"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 80685\n",
            "Training set size: 56481\n",
            "Validation set size: 12102\n",
            "Testing set size: 12102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_pairs)+len(val_pairs)+len(test_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiCjQVTnRZxg",
        "outputId": "3c539c74-d9a3-460e-de70-ae1029e1b225"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80685"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\",\"\")\n",
        "strip_chars = strip_chars.replace(\"]\",\"\")\n",
        "f\"[{re.escape(strip_chars)}]\"\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "efFztovERiUh",
        "outputId": "4b020390-696b-42c8-ad53-f682d7ac8b21"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\\\\\\\\\^_`\\\\{\\\\|\\\\}\\\\~¿]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f\"{3+5}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "giosC5FZaQUt",
        "outputId": "acf485d3-631c-4dbf-cd5a-eac148162e44"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'8'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_string):\n",
        "  lowercase = tf.strings.lower(input_string)\n",
        "  return tf.strings.regex_replace(\n",
        "    lowercase, f\"[{re.escape(strip_chars)}]\",\"\")\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "  max_tokens=vocab_size,\n",
        "  output_mode=\"int\",\n",
        "  output_sequence_length=sequence_length + 1,\n",
        "  standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_sinhala_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_sinhala_texts)"
      ],
      "metadata": {
        "id": "UdKVQbKN1d1O"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "def format_dataset(eng, sin):\n",
        "  eng = source_vectorization(eng)\n",
        "  sin = target_vectorization(sin)\n",
        "  return ({\n",
        "    \"english\": eng,\n",
        "    \"sinhala\": sin[:, :-1],\n",
        "}, sin[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "  eng_texts, sin_texts = zip(*pairs)\n",
        "  eng_texts = list(eng_texts)\n",
        "  sin_texts = list(sin_texts)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((eng_texts, sin_texts))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "  return dataset.shuffle(2048).prefetch(16).cache()\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "for inputs, targets in train_ds.take(1):\n",
        "  print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "  print(f\"inputs['sinhala'].shape: {inputs['sinhala'].shape}\")\n",
        "  print(f\"targets.shape: {targets.shape}\")\n",
        "\n",
        "  inputs['english'].shape: (64, 20)\n",
        "  inputs['sinhala'].shape: (64, 20)\n",
        "  targets.shape: (64, 20)\n",
        "\n",
        "print(list(train_ds.as_numpy_iterator())[50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJgrHUyO32Fu",
        "outputId": "665cd377-3716-44cc-c299-16737b0988ae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['sinhala'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n",
            "({'english': array([[  320,    24,   533, ...,     0,     0,     0],\n",
            "       [ 5273,    19,  4932, ...,     0,     0,     0],\n",
            "       [   46,    74,    80, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [  340, 11477,  8129, ...,     0,     0,     0],\n",
            "       [  104,     2,     0, ...,     0,     0,     0],\n",
            "       [    3,   420,     5, ...,     0,     0,     0]]), 'sinhala': array([[   2,   17,  477, ...,    0,    0,    0],\n",
            "       [   2, 7915,  172, ...,    0,    0,    0],\n",
            "       [   2,    7,   66, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [   2,    1,    1, ...,    0,    0,    0],\n",
            "       [   2,  534,    3, ...,    0,    0,    0],\n",
            "       [   2,    4,   78, ...,    0,    0,    0]])}, array([[   17,   477,  1792, ...,     0,     0,     0],\n",
            "       [ 7915,   172, 11525, ...,     0,     0,     0],\n",
            "       [    7,    66,     3, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [    1,     1,  2776, ...,     0,     0,     0],\n",
            "       [  534,     3,     0, ...,     0,     0,     0],\n",
            "       [    4,    78,   161, ...,     0,     0,     0]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer encoder implemented as a subclassed Layer"
      ],
      "metadata": {
        "id": "fyzs70do5cBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention = layers.MultiHeadAttention(\n",
        "    num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.dense_proj = keras.Sequential(\n",
        "    [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "    layers.Dense(embed_dim),]\n",
        "    )\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    if mask is not None:\n",
        "      mask = mask[:, tf.newaxis, :]\n",
        "    attention_output = self.attention(\n",
        "      inputs, inputs, attention_mask=mask)\n",
        "    proj_input = self.layernorm_1(inputs + attention_output)\n",
        "    proj_output = self.dense_proj(proj_input)\n",
        "    return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "      \"embed_dim\": self.embed_dim,\n",
        "      \"num_heads\": self.num_heads,\n",
        "      \"dense_dim\": self.dense_dim,\n",
        "    })\n",
        "    return config"
      ],
      "metadata": {
        "id": "JBvxAWIj5hA1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer decoder"
      ],
      "metadata": {
        "id": "NkY3WdL-8Mxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = mask\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ],
      "metadata": {
        "id": "4GFkZsmRWLwY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional Encoding"
      ],
      "metadata": {
        "id": "Tt0C1fdH9z_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.token_embeddings = layers.Embedding(\n",
        "      input_dim=input_dim, output_dim=output_dim)\n",
        "    self.position_embeddings = layers.Embedding(\n",
        "      input_dim=sequence_length, output_dim=output_dim)\n",
        "    self.sequence_length = sequence_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start=0, limit=length, delta=1)\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    return embedded_tokens + embedded_positions\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(PositionalEmbedding, self).get_config()\n",
        "    config.update({\n",
        "    \"output_dim\": self.output_dim,\n",
        "    \"sequence_length\": self.sequence_length,\n",
        "    \"input_dim\": self.input_dim,\n",
        "    })\n",
        "    return config\n",
        ""
      ],
      "metadata": {
        "id": "FMTnq-rv-HWR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "# Define encoder inputs\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "\n",
        "# Add positional embedding to encoder inputs\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "\n",
        "# Encode input sequence\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "# Define decoder inputs\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"sinhala\")\n",
        "\n",
        "# Add positional embedding to decoder inputs\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "\n",
        "# Decode input sequence\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "\n",
        "# Apply dropout\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Generate decoder outputs\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "# Define the transformer model\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL3DdDSBWh2u",
        "outputId": "4463f8fe-f14c-4346-a96f-5c6f23a92fb3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " english (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " sinhala (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " positional_embedding (Posi  (None, None, 256)            3845120   ['english[0][0]']             \n",
            " tionalEmbedding)                                                                                 \n",
            "                                                                                                  \n",
            " positional_embedding_1 (Po  (None, None, 256)            3845120   ['sinhala[0][0]']             \n",
            " sitionalEmbedding)                                                                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Trans  (None, None, 256)            3155456   ['positional_embedding[0][0]']\n",
            " formerEncoder)                                                                                   \n",
            "                                                                                                  \n",
            " transformer_decoder (Trans  (None, None, 256)            5259520   ['positional_embedding_1[0][0]\n",
            " formerDecoder)                                                     ',                            \n",
            "                                                                     'transformer_encoder[0][0]'] \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, None, 256)            0         ['transformer_decoder[0][0]'] \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, None, 15000)          3855000   ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19960216 (76.14 MB)\n",
            "Trainable params: 19960216 (76.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tVuWzPLbc3fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "  optimizer=\"rmsprop\",\n",
        "  loss=\"sparse_categorical_crossentropy\",\n",
        "  metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=10, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpGKrN50WoXX",
        "outputId": "7af5af40-1e5d-4eb7-ffdb-35db7ec096e0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "883/883 [==============================] - 83s 83ms/step - loss: 4.6883 - accuracy: 0.4019 - val_loss: 3.8546 - val_accuracy: 0.4683\n",
            "Epoch 2/10\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 3.9213 - accuracy: 0.4673 - val_loss: 3.5263 - val_accuracy: 0.5006\n",
            "Epoch 3/10\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 3.6154 - accuracy: 0.4955 - val_loss: 3.3727 - val_accuracy: 0.5147\n",
            "Epoch 4/10\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 3.4335 - accuracy: 0.5150 - val_loss: 3.3356 - val_accuracy: 0.5185\n",
            "Epoch 5/10\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 3.2984 - accuracy: 0.5329 - val_loss: 3.2778 - val_accuracy: 0.5289\n",
            "Epoch 6/10\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 3.1933 - accuracy: 0.5474 - val_loss: 3.3306 - val_accuracy: 0.5307\n",
            "Epoch 7/10\n",
            "883/883 [==============================] - 64s 73ms/step - loss: 3.1111 - accuracy: 0.5595 - val_loss: 3.3090 - val_accuracy: 0.5297\n",
            "Epoch 8/10\n",
            "883/883 [==============================] - 61s 69ms/step - loss: 3.0441 - accuracy: 0.5703 - val_loss: 3.3813 - val_accuracy: 0.5327\n",
            "Epoch 9/10\n",
            "883/883 [==============================] - 64s 72ms/step - loss: 2.9857 - accuracy: 0.5798 - val_loss: 3.3426 - val_accuracy: 0.5365\n",
            "Epoch 10/10\n",
            "883/883 [==============================] - 60s 68ms/step - loss: 2.9390 - accuracy: 0.5885 - val_loss: 3.3666 - val_accuracy: 0.5364\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a00905c7340>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "sin_vocab = target_vectorization.get_vocabulary()\n",
        "sin_index_lookup = dict(zip(range(len(sin_vocab)), sin_vocab))\n",
        "max_decoded_sentence_length = 20"
      ],
      "metadata": {
        "id": "2UqodT9pd77O"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_sentence):\n",
        "  tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "  decoded_sentence = \"[start]\"\n",
        "  for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sentence = target_vectorization(\n",
        "      [decoded_sentence])[:, :-1]\n",
        "    predictions = transformer(\n",
        "      [tokenized_input_sentence, tokenized_target_sentence])\n",
        "    sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "    sampled_token = sin_index_lookup[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token\n",
        "    if sampled_token == \"[end]\":\n",
        "      break\n",
        "  return decoded_sentence"
      ],
      "metadata": {
        "id": "kjIFm4RXuFhv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "  input_sentence = random.choice(test_eng_texts)\n",
        "  print(\"-\")\n",
        "  print(input_sentence)\n",
        "  print(decode_sequence(input_sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34yqMwPnd9Rb",
        "outputId": "e8641994-1596-4787-db5b-3dfac63f2172"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "වාව් වාව් වාව් වාව් මම මත්ද්‍රව්‍ය අරගෙන නෑ අපිට සාක්‍ෂි තියෙනවා\n",
            "[start] [UNK] [UNK] නෑ [UNK] මම [UNK] [UNK] නෑ ඒක [UNK] [UNK] [end]\n",
            "-\n",
            "ok the cold\n",
            "[start] හරි උදේ [end]\n",
            "-\n",
            "trying on the flight  \n",
            "[start] උත්සාහ කරන්න [end]\n",
            "-\n",
            "but there are chain murders in mumbai\n",
            "[start] ඒත් මෙතන ඉන්නවා සාමාන්‍ය [UNK] [UNK] [end]\n",
            "-\n",
            "my goodness\n",
            "[start] දෙවියනේ [end]\n",
            "-\n",
            "nathan\n",
            "[start] දැනටමත් [end]\n",
            "-\n",
            "i don't have time for this right  \n",
            "[start] මට මේක වෙලාවක් නෑ හරිද [end]\n",
            "-\n",
            "what are the gains and losses    \n",
            "[start] මොකද [UNK] [UNK] [UNK] [end]\n",
            "-\n",
            "besides he's still a coward\n",
            "[start] අනික එයා තාමත් [UNK] [end]\n",
            "-\n",
            "you didn't hear did you  \n",
            "[start] ඔයා [UNK] නේද [end]\n",
            "-\n",
            "what woman could that be stoddard  \n",
            "[start] ඒ [UNK] වෙන්න පුළුවන් ඒ [UNK] [end]\n",
            "-\n",
            "ok\n",
            "[start] හරි [end]\n",
            "-\n",
            "why don't you listen to me  \n",
            "[start] ඇයි ඔයා මට කියන දේ [end]\n",
            "-\n",
            "sacrificing lives   or equivalent knowledge  \n",
            "[start] [UNK] [UNK] [UNK] [UNK] [UNK] [end]\n",
            "-\n",
            "are you comparing me to the dog  \n",
            "[start] ඔයා මට මගේ බල්ලා එක්ක [end]\n",
            "-\n",
            " where are we    colosseum\n",
            "[start] අපි කොහෙද අපි [end]\n",
            "-\n",
            "but he didn't understand why\n",
            "[start] ඒත් එයාට තේරෙන්නේ නෑ ඇයි කියලා [end]\n",
            "-\n",
            "your baby is the first born in our new home\n",
            "[start] ඔයාගෙ දරුවා අපේ අළුත් ගෙදර අළුත් [UNK] [end]\n",
            "-\n",
            "there are a lot of braggarts here\n",
            "[start] මෙතන ගොඩක් තියෙනවා [UNK] [end]\n",
            "-\n",
            "whatever it is he is not the taxman\n",
            "[start] එයා මොකක් වුනත් එයා [UNK] නෑ [end]\n"
          ]
        }
      ]
    }
  ]
}